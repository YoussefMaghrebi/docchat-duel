from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import difflib
import os

def load_llm_model(model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'):
    """
    Loads a pre-trained language model and its tokenizer.

    Args:
        model_name (str): Hugging Face model identifier. Defaults to 'tiiuae/falcon-rw-1b'.

    Returns:
        tuple: (tokenizer, model)
            - tokenizer (PreTrainedTokenizer): Tokenizer for the LLM.
            - model (PreTrainedModel): Loaded language model ready for inference.
    
    Notes:
        - Automatically uses GPU if available.
        - If memory is limited, weights are offloaded to disk (./offload).
    """   

     # Convert model name to a safe folder name
    safe_model_name = model_name.replace("/", "_")

    offload_dir = os.path.join("./offload", safe_model_name)

    if torch.cuda.is_available():
        device_map = {"": 0}     # device set to GPU
        print('using cuda for LLM model')
        dtype = torch.float16
    else:
        device_map = "auto"      # "auto" uses the accelerate library, which manages how the model is split between CPU/GPU  
        dtype = torch.float32

    model_name = model_name
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype= dtype,
        device_map= device_map,          
        offload_folder= offload_dir  # folder to offload some model weights to disk when Hugging Face detects not enough GPU/CPU RAM
    )
    return tokenizer, model

def generate_prompt(chunk_texts, question):
    """
    Constructs a prompt to be fed into the language model.

    Args:
        chunk_texts (List[str]): List of query-relevant text chunks from document embeddings.
        question (str): User query.

    Returns:
        prompt (str): Formatted prompt string combining context and question.
    """

    context = "\n---\n".join(chunk_texts)
    prompt = f"""You are a helpful assistant. Carefully read the context below and answer the question fully.

                Context:
                {context}

                Question:
                {question}

                Answer:"""
    
    return prompt

def generate_answer(chunk_texts, question, tokenizer, model, max_tokens=170, top_k=50, top_p=0.9, temperature=0.7, do_sample=True):
    """
    Generates an answer from the language model using given context and question.

    Args:
        chunk_texts (List[str]): List of query-relevant text chunks from document embeddings.
        question (str): User query.
        tokenizer (PreTrainedTokenizer): Tokenizer corresponding to the LLM.
        model (PreTrainedModel): Loaded LLM.
        max_tokens (int): Maximum number of tokens to generate in the response.
        top_k:
        top_p: 
        temperature:
        do_sample: 

    Returns:
        answer (str): The generated answer text.
    """

    # construct a prompt for the LLM model
    prompt = generate_prompt(chunk_texts, question)

    # pipeline for text generation
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer
    )

    # retrieve result for the query using deterministic output (no token sampling)
    #result = pipe(prompt, max_new_tokens=max_tokens, do_sample=False)[0]["generated_text"]

    # if we want to allow token sampling and some randomness in the output generation
    result = pipe(prompt, max_new_tokens=max_tokens, top_k= top_k, top_p= top_p, temperature= temperature, do_sample= do_sample)[0]["generated_text"]
    
    # Return only the generated answer part (remove prompt text etc.) 
    answer = result.split("Answer:")[-1].strip()

    return answer

def remove_near_duplicates(response, similarity_threshold=0.9):
    """
    Removes near-duplicate lines from an LLM-generated response using difflib's SequenceMatcher.

    Args:
        response (str): The text output generated by the LLM.
        similarity_threshold (float): Similarity ratio (0 to 1) above which lines are considered duplicates. Default is 0.9.

    Returns:
        str: A cleaned version of the response with similar/redundant lines removed.
    """

    lines = response.split("\n")
    cleaned = []
    for line in lines:
        if not any(difflib.SequenceMatcher(None, line, prev).ratio() > similarity_threshold for prev in cleaned):
            cleaned.append(line)
    cleaned = "\n".join(cleaned)

    return cleaned
